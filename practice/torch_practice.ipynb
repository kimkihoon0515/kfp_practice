{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import kfp.components as components\n",
    "from kfp import dsl\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  import torch.nn as nn\n",
    "  import torch\n",
    "  from torchvision import datasets\n",
    "  from torchvision import transforms\n",
    "  from torch.utils.data import DataLoader\n",
    "  import numpy as np\n",
    "  import os\n",
    "  class Net(nn.Module): # 모델 클래스 정의 부분\n",
    "      \n",
    "      def __init__(self):\n",
    "          super(Net, self).__init__()\n",
    "          self.fc1 = nn.Linear(784,100) # MNIST 데이터셋이 28*28로 총 784개의 픽셀로 이루어져있기 때문에 784를 입력 크기로 넣음.\n",
    "          self.relu = nn.ReLU()\n",
    "          self.fc2 = nn.Linear(100,100) # 은닉층\n",
    "          self.fc3 = nn.Linear(100,10) # 출력층 0~9까지 총 10개의 클래스로 결과가 나옴.\n",
    "\n",
    "      def forward(self, x): # 입력층 -> 활성화 함수(ReLU) -> 은닉층 -> 활성화 함수(ReLU) -> 출력층\n",
    "          x1 = self.fc1(x)\n",
    "          x2 = self.relu(x1)\n",
    "          x3 = self.fc2(x2)\n",
    "          x4 = self.relu(x3)\n",
    "          x5 = self.fc3(x4)\n",
    "\n",
    "          return x5\n",
    "\n",
    "  from minio import Minio\n",
    "\n",
    "  minio_client = Minio(\n",
    "      \"172.17.0.38:9000\",\n",
    "      access_key=\"minio\",\n",
    "      secret_key=\"minio123\",\n",
    "      secure=False\n",
    "  )\n",
    "\n",
    "  minio_bucket = \"mlpipeline\"\n",
    "\n",
    "  for item in minio_client.list_objects(minio_bucket,prefix=\"mnist\",recursive=True):\n",
    "      minio_client.fget_object(minio_bucket,item.object_name,item.object_name)\n",
    "\n",
    "  train_dataset = datasets.MNIST(root=\"./mnist/\",\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=False) # 학습 dataset 정의\n",
    "                          \n",
    "  test_dataset = datasets.MNIST(root=\"./mnist/\",\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor(), \n",
    "                          download=False) # 평가 dataset 정의\n",
    "\n",
    "  batch_size = 100 # 배치 사이즈 정의. 데이터셋을 잘개 쪼개서 묶음으로 만드는 데 기여한다.\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # 학습 데이터셋을 배치 사이즈 크기만큼씩 잘라서 묶음으로 만든다. 묶음의 개수는 train_dataset / batch_size\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # train_dataloader와 마찬가지\n",
    "\n",
    "  model = Net() # 모델 정의\n",
    "  loss_function = nn.CrossEntropyLoss() # 실제 정답과 예측값의 차이를 수치화해주는 함수.\n",
    "\n",
    "  optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "  epochs = 10# 얼마나 학습할 지 정하는 인자.\n",
    "\n",
    "  best_accuracy = 0 # 평가 지표\n",
    "  model.zero_grad() # 학습 전에 모델의 모든 weight, bias 값들을 초기화\n",
    "\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "    \n",
    "    model.train() # 학습\n",
    "    train_accuracy = 0 # metric\n",
    "    train_loss = 0 # metric\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "      images = images.reshape(batch_size,784)\n",
    "      image = model(images)\n",
    "      loss = loss_function(image,labels)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      prediction = torch.argmax(image,1)\n",
    "      correct = (prediction == labels)\n",
    "      train_accuracy+= correct.sum().item() / len(train_dataset)\n",
    "      train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    model.eval() # 평가\n",
    "    val_accuracy = 0 # metric\n",
    "    val_loss = 0 # metric\n",
    "\n",
    "    for images,labels in test_loader:\n",
    "      images = images.reshape(batch_size,784)\n",
    "      image = model(images)\n",
    "      loss = loss_function(image,labels)\n",
    "      \n",
    "      correct = (torch.argmax(image,1) == labels)\n",
    "      val_accuracy += correct.sum().item() / len(test_dataset)\n",
    "      val_loss += loss.item() / len(test_loader)\n",
    "    \n",
    "    print(f'epoch: {epoch}/{epochs} train_loss: {train_loss:.5} train_accuracy: {train_accuracy:.5} val_loss: {val_loss:.5} val_accuracy: {val_accuracy:.5}')\n",
    "\n",
    "    if best_accuracy < val_accuracy: # 성능이 가장 좋은 모델로 갱신\n",
    "      best_accuracy = val_accuracy\n",
    "      torch.save(model.state_dict(),'best_model.pt')\n",
    "      print(f\"===========> Save Model(Epoch: {epoch}, Accuracy: {best_accuracy:.5})\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "  \n",
    "  minio_client.fput_object(minio_bucket,\"best_model.pt\",\"./best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/10 train_loss: 0.70496 train_accuracy: 0.80235 val_loss: 0.29239 val_accuracy: 0.916\n",
      "===========> Save Model(Epoch: 0, Accuracy: 0.916)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 1/10 train_loss: 0.256 train_accuracy: 0.9256 val_loss: 0.2008 val_accuracy: 0.9402\n",
      "===========> Save Model(Epoch: 1, Accuracy: 0.9402)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 2/10 train_loss: 0.18473 train_accuracy: 0.9465 val_loss: 0.17032 val_accuracy: 0.9507\n",
      "===========> Save Model(Epoch: 2, Accuracy: 0.9507)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 3/10 train_loss: 0.1445 train_accuracy: 0.9588 val_loss: 0.1326 val_accuracy: 0.9589\n",
      "===========> Save Model(Epoch: 3, Accuracy: 0.9589)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 4/10 train_loss: 0.11798 train_accuracy: 0.96633 val_loss: 0.11238 val_accuracy: 0.9663\n",
      "===========> Save Model(Epoch: 4, Accuracy: 0.9663)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 5/10 train_loss: 0.098811 train_accuracy: 0.97108 val_loss: 0.11113 val_accuracy: 0.9656\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 6/10 train_loss: 0.085606 train_accuracy: 0.975 val_loss: 0.09631 val_accuracy: 0.9709\n",
      "===========> Save Model(Epoch: 6, Accuracy: 0.9709)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 7/10 train_loss: 0.074475 train_accuracy: 0.97818 val_loss: 0.10524 val_accuracy: 0.9673\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 8/10 train_loss: 0.065443 train_accuracy: 0.98113 val_loss: 0.090854 val_accuracy: 0.9722\n",
      "===========> Save Model(Epoch: 8, Accuracy: 0.9722)\n",
      "--------------------------------------------------------------------------------------------\n",
      "epoch: 9/10 train_loss: 0.058956 train_accuracy: 0.98243 val_loss: 0.085044 val_accuracy: 0.9731\n",
      "===========> Save Model(Epoch: 9, Accuracy: 0.9731)\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train = components.create_component_from_func(train,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-pytorch:v1.5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"mnist-train\",\n",
    "    description=\"train mnist datasets downloaded from minio\"\n",
    ")\n",
    "def train_pipeline():\n",
    "    comp_train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(pipeline_func=train_pipeline,package_path='output_test.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track test loss\n",
    "\n",
    "def test():\n",
    "    model = Net()\n",
    "    test_loss     = 0.0\n",
    "    class_correct = [0]*10\n",
    "    class_total   = [0]*10\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # For generating confusion matrix\n",
    "    conf_matrix = np.zeros((10,10))\n",
    "\n",
    "    # iterate over test data\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        images = data.reshape(batch_size,784)\n",
    "        image = model(images)\n",
    "        loss = criterion(image, target)\n",
    "        # update test loss \n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(image, 1)    \n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy())\n",
    "        test_loss += loss / len(test_loader)\n",
    "        # calculate test accuracy for each object class\n",
    "\n",
    "        for i in range(target.size(0)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "            \n",
    "            # Update confusion matrix\n",
    "            conf_matrix[label][pred.data[i]] += 1\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(test_loader)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %3s: %2d%% (%2d/%2d)' % (\n",
    "                i, 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %3s: N/A (no training examples)' % (class_total[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.subplots(figsize=(10,9))\n",
    "    ax = sns.heatmap(conf_matrix, annot=True, vmax=20)\n",
    "    ax.set_xlabel('Predicted');\n",
    "    ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.023036\n",
      "\n",
      "Test Accuracy of   0:  0% ( 0/980)\n",
      "Test Accuracy of   1: 99% (1133/1135)\n",
      "Test Accuracy of   2:  0% ( 0/1032)\n",
      "Test Accuracy of   3:  0% ( 3/1010)\n",
      "Test Accuracy of   4:  0% ( 0/982)\n",
      "Test Accuracy of   5:  0% ( 0/892)\n",
      "Test Accuracy of   6:  0% ( 0/958)\n",
      "Test Accuracy of   7:  1% (15/1028)\n",
      "Test Accuracy of   8:  0% ( 0/974)\n",
      "Test Accuracy of   9:  0% ( 0/1009)\n",
      "\n",
      "Test Accuracy (Overall): 11% (1151/10000)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
